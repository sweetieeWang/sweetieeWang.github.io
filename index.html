<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="SweetieeWang&#39;s Page">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="SweetieeWang&#39;s Page">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jing Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>SweetieeWang's Page</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">SweetieeWang's Page</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/12/machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Author">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SweetieeWang's Page">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/12/machine-learning/" class="post-title-link" itemprop="url">machine_learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2023-04-12 15:54:19 / Modified: 15:55:08" itemprop="dateCreated datePublished" datetime="2023-04-12T15:54:19+08:00">2023-04-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="机器学习概论">机器学习概论</h1>
<h2 id="数学知识复习">数学知识复习</h2>
<h3 id="矩阵求导">矩阵求导</h3>
<p><span class="math display">\[
\begin{array}{l}\frac{\partial \mathbf{x}^{T} \mathbf{a}}{\partial
\mathbf{x}}=\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial
\mathbf{x}}=\mathbf{a} \\ \frac{\partial \mathbf{a}^{T} \mathbf{X}
\mathbf{b}}{\partial \mathbf{X}}=\mathbf{a b}^{T} \\ \frac{\partial
\mathbf{a}^{T} \mathbf{X}^{T} \mathbf{b}}{\partial \mathbf{X}}=\mathbf{b
a}^{T} \\ \frac{\partial \mathbf{a}^{T} \mathbf{X} \mathbf{a}}{\partial
\mathbf{X}}=\frac{\partial \mathbf{a}^{T} \mathbf{X}^{T}
\mathbf{a}}{\partial \mathbf{X}}=\mathbf{a a}^{T} \\ \frac{\partial
\mathbf{x}^{T} \mathbf{B} \mathbf{x}}{\partial
\mathbf{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right) \mathbf{x}\end{array}
\]</span></p>
<figure>
<img src="/Users/wangjing/Downloads/机器学习..png" alt="机器学习." />
<figcaption aria-hidden="true">机器学习.</figcaption>
</figure>
<h1 id="多元线性回归">多元线性回归</h1>
<h3 id="函数模型">函数模型</h3>
<p><strong>函数形式</strong> <span class="math display">\[
f(x)=\theta_{0}+\theta_{1} x_{1}+\cdots+\theta_{p} x_{p}
\]</span> <strong>向量形式</strong>：</p>
<p>通常一个向量指的都是列向量，向量的转置是行向量 <span
class="math display">\[
f(x)=\sum_{i=0}^{p} \theta_{i} x_{i}=\boldsymbol{\theta}^{T} x=x^{T}
\boldsymbol{\theta} = \left[\left(x_{0}=1\right), x_{1}, x_{2}, \ldots,
x_{p}\right]\left[\begin{array}{c}\theta_{0} \\ \theta_{1} \\ \vdots \\
\theta_{p}\end{array}\right]
\]</span> 损失函数：最小均方误差MSE： <span class="math display">\[
J(\theta)=\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}^{T}
\theta-y_{i}\right)^{2}
\]</span> 线性回归模型：求解损失函数的最小值 <span
class="math display">\[
\theta^* = arg minJ(\theta)
\]</span></p>
<h3 id="加入数据后的模型">加入数据后的模型</h3>
<p>n组数据</p>
<p>预测值： <span class="math display">\[
\hat Y = X\theta=\left[\begin{array}{l} X_1^T\theta \\X_2^T\theta \\
\ldots \\X_n^T\theta \\  \end{array}\right]=\left[\begin{array}{l}
X_{11}\space X_{12}\ldots X_{1p}\\X_{21}\space X_{22}\ldots X_{2p} \\
\ldots \\X_{n1}\space X_{n2}\ldots X_{np}
\\\end{array}\right]\left[\begin{array}{c}\theta_{0} \\ \theta_{1} \\
\vdots \\ \theta_{p}\end{array}\right]
\]</span> 实际值label (n组数据n个label)： <span class="math display">\[
Y =\left[\begin{array}{c}y_1 \\ y_2\\ \vdots \\ y_n\end{array}\right]
\]</span></p>
<h3 id="模型求解">模型求解</h3>
<h4 id="梯度下降法">梯度下降法</h4>
<p>Gradient Decent <span class="math display">\[
\theta:=\theta-\alpha \nabla_{\theta} J(\theta)
\]</span></p>
<p><span class="math display">\[
J(\theta)=\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}^{T}
\theta-y_{i}\right)^{2}
\]</span></p>
<p>其中算子：梯度是偏导数的自然扩展 <span class="math display">\[
\nabla_{\theta} J=\left[\begin{array}{l}\frac{\partial J}{\partial
\theta_{0}} \\ \cdots \\ \cdots  \\ \frac{\partial J}{\partial
\theta_{p}}\end{array}\right]
\]</span> 求损失函数的偏导： <span class="math display">\[
\begin{array}{l}\frac{\partial 1}{\theta_{j} 2}\left(x_{i}^{T}
\theta-y_{i}\right)^{2} \\ =\frac{\partial 1}{\theta_{j}
2}\left(\sum_{j=0}^{p} x_{i, j} \theta_{j}-y_{i}\right)^{2} \quad
x_{i}=\left(x_{i, 0}, \ldots, x_{i, p}\right)^{T} \\
=\left(\sum_{j=0}^{p} x_{i, j} \theta_{j}-y_{i}\right)
\frac{\partial}{\theta_{j}}\left(\sum_{j=0}^{p} x_{i, j}
\theta_{j}-y_{i}\right) \\ =\left(f\left(x_{i}\right)-y_{i}\right) x_{i,
j}\end{array}
\]</span></p>
<h4 id="正规方程法">正规方程法</h4>
<p><span class="math display">\[
\begin{aligned} J(\theta) &amp;=\frac{1}{2}\|Y-X \theta\|^{2} \\
&amp;=\frac{1}{2}(X \theta-Y)^{T}(X \theta-Y) \\
&amp;=\frac{1}{2}\left(\theta^{T} X^{T} X \theta-2 Y^{T} X \theta+Y^{T}
Y\right) \end{aligned}
\]</span></p>
<p>注解： <span class="math display">\[
\begin{array}{l}\frac{\partial \mathbf{x}^{T} \mathbf{B}
\mathbf{x}}{\partial \mathbf{x}}=\left(\mathbf{B}+\mathbf{B}^{T}\right)
\mathbf{x} \\ \frac{\partial \mathbf{x}^{T} \mathbf{a}}{\partial
\mathbf{x}}=\frac{\partial \mathbf{a}^{T} \mathbf{x}}{\partial
\mathrm{x}}=\text { a }\\\end{array}
\]</span> 我们令<span class="math inline">\(B=X^TX,B^T=B\Longrightarrow
(B+B^B)\theta = 2B\theta\)</span> <span class="math display">\[
\nabla_{\theta} J(\theta)=\frac{\partial J(\theta)}{\partial
\theta}=\frac{\frac{1}{2}\left(\theta^{T} X^{T} X \theta-2 Y^{T} X
\theta+Y^{T} Y\right)}{\partial \theta}=X^{T} X \theta-\left(Y^{T}
X\right)^{T}=X^{T} X \theta-X^{T} Y=0\\\Longrightarrow X^{T} X
\theta=X^{T} Y\theta^{*}=\left(X^{T} X\right)^{-1}
X^{T}\\\Longrightarrow\theta^{*}=\left(X^{T} X\right)^{-1} X^{T} Y
\]</span></p>
<h4 id="随机梯度下降法">随机梯度下降法</h4>
<p>Mini-batch GD</p>
<p>每次只 用训练集中的一个数据，把数据分为若干个批，按批来更新参
数。一个批中的一组数据共同决定了本次梯度的方向，下降起
来就不容易跑偏，减少了随机性。</p>
<p>一个bacth 形成一个epoch分批次训练</p>
<h3 id="全局最优解">全局最优解</h3>
<p>当<span
class="math inline">\(J(\theta)\)</span>是凸函数（凹函数和凸函数统称凸函数）时，二阶导数大于0,<span
class="math inline">\(X^TX\)</span>为半正定矩阵 <span
class="math display">\[
\nabla_{\theta}^{2} J(\theta)=X^{T} X
\]</span> 当训练样本的数目n大于训练样本的维度（p+1 个属性，特征）<span
class="math inline">\(X^TX\)</span>通常可逆，表明改矩阵事正定矩阵，求的参数是全局最优解。不可逆时，可以接出多个参数解。可使用
正则化给出一个“归纳偏好”解。</p>
<h3 id="评估方法">评估方法</h3>
<h4 id="留出法">留出法</h4>
<p>随机挑选 一部分标 记数据作 为测试集 (空心点 )，其余的作 为训练集
(实心点 )，计算 回归模型，使用测试 集对模型 评估: MSE
=2.4，测试集不能太大，也不 能太小。2 &lt;= n:m &lt;=4</p>
<h4 id="交叉验证法">交叉验证法</h4>
<p><img
src="https://cdn.mathpix.com/snip/images/nXRmmZcFN_wIuR7Nc-faI45CWKH5hS6nU-eZ3hlYD70.original.fullsize.png" /></p>
<h4 id="性能度量">性能度量</h4>
<h5 id="线性回归模型平方和误差">线性回归模型：平方和误差</h5>
<p>在测试集上报告 MSE(mean square error) 误差 <span
class="math display">\[
J_{\text {train }}(\theta)=\frac{1}{2}
\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{T} \theta-y_{i}\right)^{2}
\]</span></p>
<p><span class="math display">\[
\theta^{*}=\operatorname{argmin} J_{\text {train
}}(\theta)=\left(X_{\text {train }}^{T} X_{\text {train }}\right)^{-1}
X_{\text {train }}^{T} \vec{y}_{\text {train }}
\]</span></p>
<p><span class="math display">\[
J_{\text {test }}=\frac{1}{m} \sum_{i=n+1}^{n+m}\left(\mathbf{x}_{i}^{T}
\theta^{*}-y_{i}\right)^{2}=\frac{1}{m} \sum_{i=n+1}^{n+m}
\varepsilon_{i}^{2}
\]</span></p>
<h5 id="分类任务错误率与精度">分类任务：错误率与精度</h5>
<p>错误率是分类错误的样本数占样本总数的比例</p>
<p>精度是分类正确的样本数占样本总数的比例</p>
<p>对二分类问题：</p>
<p>查准率：<span class="math inline">\(P=\frac{T P}{T P+F
P}\)</span></p>
<p>查全率：<span class="math inline">\(R=\frac{T P}{T P+F
N}\)</span></p>
<p>F1: <span class="math display">\[
F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数
}+T P-T N}
\]</span></p>
<h2 id="基于非线形基的线性回归">基于非线形基的线性回归</h2>
<h3 id="多项式回归">多项式回归</h3>
<h1 id="lr-逻辑回归">LR-逻辑回归</h1>
<h2 id="structural-model">Structural model</h2>
<p>逻辑函数（logistic/sigmoid function）</p>
<p><span class="math display">\[
y=\frac{1}{1+e^{-z}} = \frac{1}{1+e^{-\theta x}}
\]</span></p>
<h2 id="error-model">Error model</h2>
<p>损失函数 Loss function</p>
<p><span class="math display">\[
\begin{array}{c}P(y=1 \mid x ;
\theta)=f_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} * x}} \\ P(y=0 \mid x ;
\theta)=1-f_{\theta}(x)=\frac{e^{-\theta^{T} * x}}{1+e^{-\theta^{T} *
x}}\end{array}
\]</span></p>
<p>求参数方法--对<span
class="math inline">\(\theta\)</span>极大似然估计，使得y发生的概率最大
<span class="math display">\[
L(\theta)=\prod_{i=1}^{n} P\left(y_{i} \mid x_{i} ;
\theta\right)=\prod_{i=1}^{n}\left(f_{\theta}\left(x_{i}\right)\right)^{y_{i}}\left(1-f_{\theta}\left(x_{i}\right)\right)^{1-y_{i}}
\]</span> 转化为对数函数，将<span
class="math inline">\(\frac{f_{\theta}(x)}
{1-f_{\theta}(x)}=\frac{1}{1+e^{-\theta^{T} * x}}/\frac{e^{-\theta^{T} *
x}}{1+e^{-\theta^{T} * x}} = \frac{1}{e^{-\theta x}} = e^{\theta
x}\)</span> <span class="math display">\[
\begin{array}{l}\ln L(\theta)=\sum_{i=1}^{n}\left(y_{i} \ln
\left(f_{\theta}\left(x_{i}\right)\right)+\left(1-y_{i}\right) \ln
\left(1-f_{\theta}\left(x_{i}\right)\right)\right) \\
=\sum_{i=1}^{n}\left(\left(1-y_{i}\right)\left(-\theta^{T} *
x_{i}\right)-\ln \left(1+e^{-\theta^{T} *
x_{i}}\right)\right)\end{array}
\]</span> 梯度上升 <span class="math display">\[
\theta:=\theta+\alpha \nabla_{\theta} \ln (L(\theta)) \Leftrightarrow
\theta_{j}:=\theta_{j}+\frac{\partial \ln (L(\theta))}{\partial
\theta_{j}}
\]</span> 求梯度 <span class="math display">\[
\begin{aligned} \nabla_{\theta} \ln (L(\theta))
&amp;=\sum_{i=1}^{n}\left[-\left(1-y_{i}\right) \cdot
x_{i}-\frac{1}{1+e^{-\theta^{T} x_{i}}}\left(e^{-\theta^{T}
x_{i}}\right)\left(-x_{i}\right)\right] \\
&amp;=\sum_{i=1}^{n}\left(-1+y_{i}+\frac{e^{-\theta^{T}
x_{i}}}{1+e^{-\theta^{T} x_{i}}}\right) x_{i} \\
&amp;=\sum_{i=1}^{n}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)
x_{i} \Leftrightarrow \frac{\partial}{\partial \theta_{j}} \ln
(L(\theta))=\sum_{i=1}^{n}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)
x_{i, j} \end{aligned}
\]</span> 代入梯度的参数更新 <span class="math display">\[
\theta:=\theta+\alpha \nabla_{\theta} \ln (L(\theta)) \Rightarrow
\theta:=\theta+\alpha
\sum_{i=1}^{n}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right) x_{i}
\]</span> ## 和线性回归的对比</p>
<p>和线形回归模型看似一样，但是f不同，逻辑回归解决的是二分类问题</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>逻辑回归</th>
<th>线性回归</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>输出</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>线形二分类</td>
<td>线性拟合</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="nn-神经网络">NN-神经网络</h1>
<h2 id="structural-model-1">Structural model</h2>
<h3 id="逻辑回归的二阶段表示">逻辑回归的二阶段表示</h3>
<p><span class="math inline">\(z = b+ \sum x_iw = \mathop{W^T}\limits_{p
\times 1}\mathop{x}\limits_{1\times p} +
\mathop{b}\limits_{1\times1}\)</span></p>
<p><span class="math inline">\(\hat y = sigmoid(z) =
\frac{e^z}{1+e^z}\)</span></p>
<h3 id="神经元">神经元</h3>
<p>神经元=线性组合(z，接收信号)+非线性激活(sigmoid， 输出非线性决策面)
<span class="math display">\[
\boldsymbol{z}_{t}=W_{1}^{T} \boldsymbol{x}
\]</span> 多神经元</p>
<p>神经网络包含多个神经元， 输入x与多个神经元相连。</p>
<h3 id="一个隐藏层的神经网络">一个隐藏层的神经网络</h3>
<p><span class="math display">\[
\boldsymbol{z}_{1}=W_{1}^{T} \boldsymbol{x}\\h_1 = sigmoid(z_1)\\z_2 =
w_2^Th_1\\ \hat y = sigmoid(z_1)
\]</span> W表示X的第j个元素与向量Z的第i个元素之间的链接权重 <span
class="math display">\[
W=\left[\begin{array}{llll}W_{11} &amp; W_{21} &amp; W_{31} &amp; W_{41}
\\ W_{12} &amp; W_{22} &amp; W_{32} &amp; W_{42} \\ W_{13} &amp; W_{23}
&amp; W_{33} &amp; W_{43}\end{array}\right]
\]</span></p>
<p><span class="math display">\[
\mathrm{W}^{T}=\left[\begin{array}{lll}W_{11} &amp; W_{12} &amp; W_{13}
\\ W_{21} &amp; W_{22} &amp; W_{23} \\ W_{31} &amp; W_{32} &amp; W_{33}
\\ W_{41} &amp; W_{42} &amp; W_{43}\end{array}\right]
\]</span></p>
<p>隐含层h 没有隐含层就只需要一个列向量，因为有隐含层所以需要W矩阵
每一层计算就是线性组合+非线形激活</p>
<h3 id="非线形激活函数">非线形激活函数</h3>
<p>引入非线性激活函数的目的是得到非线性决策面，非线形激活函数可以逼近任何复杂的函数，不论网络多深，线形函数只能输出线性决策面。</p>
<p>非线形激活函数
Relu效果最好，因为有部分导数为0，有些为1，为0的部分可以让有些神经元停止学习，起到dropout的作用，可以有效防止过拟合。</p>
<p>binary step <span class="math display">\[
f(x)=\left\{\begin{array}{lll}0 &amp; \text { for } &amp; x&lt;0 \\ 1
&amp; \text { for } &amp; x \geq 0\end{array}\right.
\]</span> Logistic <span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}
\]</span> Tanh <span class="math display">\[
f(x)=\tanh (x)=\frac{2}{1+e^{-2 x}}-1
\]</span> ReLU <span class="math display">\[
f(x)=\left\{\begin{array}{lll}0 &amp; \text { for } &amp; x&lt;0 \\ x
&amp; \text { for } &amp; x \geq 0\end{array}\right.
\]</span></p>
<h3 id="多分类神经网络">多分类神经网络</h3>
<p><img src="/Users/wangjing/Library/Application Support/typora-user-images/image-20211019140804340.png" alt="image-20211019140804340" style="zoom:45%;" />
<span class="math display">\[
\begin{array}{l}\boldsymbol{z}_{1}=\boldsymbol{W}_{1}^{T} \boldsymbol{x}
\\ h_{1}=\operatorname{sigmoid}\left(z_{1}\right) \\
z_{2}=\boldsymbol{W}_{2}^{T} h_{1} \\
h_{2}=\operatorname{sigmoid}\left(z_{2}\right) \\
\boldsymbol{z}_{3}=w_{3}^{T} h_{2} \\
\hat{y}=\operatorname{sigmoid}\left(z_{3}\right)\end{array}
\]</span> <span class="math inline">\(h_1\)</span>表示hidden layer 1
output</p>
<p>Hidden layer(隐层)的个数大于1的神经网络，称为深度神经网络</p>
<h2 id="error-model-1">Error model</h2>
<p>非正确预测导致的代价</p>
<h3 id="loss-function">Loss function</h3>
<p>交叉熵函数（cross entropy loss）</p>
<h4 id="二分类损失">二分类损失</h4>
<p>逻辑回归中，使用对数似然度量损失(每个样本属于其真实
标记的概率越大越好) <span class="math display">\[
\begin{aligned} E=\operatorname{loss} &amp;=-\log P(\mathrm{Y}=\hat{y}
\mid \mathbf{X}=\boldsymbol{x}) \\ &amp;=-y \log (\hat{y})-(1-y) \log
(1-\hat{y}) \end{aligned}
\]</span></p>
<h4 id="多分类损失">多分类损失</h4>
<h5 id="softmax函数">Softmax函数</h5>
<p>(柔性 最大值):将输出值转化成概率。 <span class="math display">\[
\hat{y}_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{K}
e^{z_{j}}}=\mathrm{P}\left(y_{i}=1 \mid \mathrm{x}\right)
\]</span> <span class="math inline">\(y_j\)</span>为one-hot
向量,真实标签位置1其他位置为0 <span class="math display">\[
E=\operatorname{loss}=-\sum_{j=1 . K} y_{j} \log \hat{y}_{j}
\]</span></p>
<h4 id="回归损失">回归损失</h4>
<p>与分类网络不同：输出层（最后一层）不再包含sigmoid函数</p>
<h5 id="二次代价函数">二次代价函数</h5>
<p><span class="math display">\[
\begin{array}{l}E=\operatorname{Los} s=\frac{1}{2}\|y-\hat{y}\|^{2} \\
=\frac{1}{2} \sum_{j=1}^{K}\left(y_{j}-\hat{y}_{j}\right)^{2}\end{array}
\]</span></p>
<h2 id="模型建模">模型建模</h2>
<p>优化参数目标:寻找使损失达到最小的神经网络权重 <span
class="math display">\[
\mathrm{W}^{*}=\underset{W}{\operatorname{argmin}} E(\hat{y} ;
\mathrm{W})
\]</span> 如何学习实现目标的神经网络权重𝑊 --梯度下降 <span
class="math display">\[
W_{L}(t+1)=W_{L}(t)-\eta \frac{\partial E}{\partial W_{L}(t)}
\]</span> ### 反向传播</p>
<p>求偏导从而应用梯度下降</p>
<ol type="1">
<li>重复应用微积分的链式法则</li>
<li>局部最小化目标函数</li>
<li>要求网络所有的“块”(blocks)都是可微的</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正向计算--节点</span><br><span class="line">反向求导--边 链式法则从后往前求</span><br></pre></td></tr></table></figure>
<h4 id="反向传播--回归实例">反向传播--回归实例</h4>
<p>回归损失函数为二次代价函数 <span class="math display">\[
E = loss = \frac{1}{2}(y-\hat y)^2
\]</span></p>
<h4 id="反向传播--二分类实例">反向传播--二分类实例</h4>
<p>二分类损失函数为交叉熵损失函数 <span class="math display">\[
\text { Loss }=-y \ln (\widehat{y})-(1-y) \ln (1-\widehat{y})
\]</span> 通过梯度下降 最小化Loss <span class="math display">\[
\begin{array}{l}w_{2}(t+1)=w_{2}(t)-\eta \frac{\partial E}{\partial
w_{2}(t)} \\ W_{1}(t+1)=W_{1}(t)-\eta \frac{\partial E}{\partial
W_{1}(t)}\end{array}
\]</span></p>
<p>函数关于一个矩阵求偏导--&gt;对每一个元素求偏导,<span
class="math inline">\(W_{11}^1\)</span>表示输入x的第j个元素到第一个隐层的第i个神经元的权重
<span class="math display">\[
\begin{aligned} E=&amp;-y \ln (\hat{y}) -(1-y) \ln (1-\hat{y}) \\
\hat{y}=&amp; \frac{e^{z_{2}}}{1+e^{z_{2}}} \\ z_{2}=&amp;
\boldsymbol{w}_{2}^{T} \boldsymbol{h}_{1} \\ \boldsymbol{h}_{1}=&amp;
\frac{e^{z_{1}}}{1+e^{z_{1}}} \\ \boldsymbol{z}_{1}=&amp; W_{1}^{T}
\boldsymbol{x} \end{aligned}
\]</span></p>
<p><span class="math display">\[
\frac{\partial E}{\partial \boldsymbol{W}_{1}}=\frac{\partial
E}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_{2}} \cdot
\mid \frac{\partial z_{2}}{\partial \boldsymbol{h}_{1}} \cdot
\frac{\partial \boldsymbol{h}_{1}}{\partial \boldsymbol{z}_{1}} \cdot
\frac{\partial \boldsymbol{z}_{1}}{\partial W_{1}}
\]</span></p>
<p><strong>Hadamard (哈达玛)乘积 /schur 乘积</strong> 假设
𝑠和𝑡是两个同样维度的向量，使用𝑠 ∘ 𝑡(或𝑠 ⊙ 𝑡)来表示按元素的乘积: <span
class="math inline">\((𝑠⊙𝑡) =s_jt_j\)</span> <span
class="math display">\[
\left[\begin{array}{l}1 \\ 2\end{array}\right]
\odot\left[\begin{array}{l}3 \\
4\end{array}\right]=\left[\begin{array}{l}1 * 3 \\ 2 *
4\end{array}\right]=\left[\begin{array}{l}3 \\ 8\end{array}\right]
\]</span></p>
<p><strong>反向传播的局部性</strong></p>
<p>反向传播的一般情形 第𝑙层第𝑗个神经元和第𝑙 − 1 层神经元之间关系 <span
class="math display">\[
z_{j}^{l}=\sum_{k=1} w_{j k}^{l-1} h_{k}^{l-1}+b_{j}^{l-1}
\]</span></p>
<h4 id="反向传播的一般情形">反向传播的一般情形</h4>
<p>一些定义</p>
<p><span class="math inline">\(\delta_{j}^{l}: \quad \delta_{j}^{l}
\equiv \frac{\partial E}{\partial
z_{j}^{l}}\)</span>，称为在第𝑙层第𝑗个神经元的误差 <span
class="math display">\[
\begin{array}{l}z_{j}^{l}=\sum_{k=1} w_{j k}^{l-1}
h_{k}^{l-1}+b_{j}^{l-1} \\ h_{j}^{l}=\sigma\left(z_{j}^{l}\right) \\
\sigma(x)=\frac{1}{1+e^{-x}}\end{array}
\]</span> 矩阵表达形式--代价函数</p>
<p><span class="math display">\[
E=\frac{1}{2}\left\|\boldsymbol{y}-\boldsymbol{h}^{L}\right\|^{2}=\frac{1}{2}\|\boldsymbol{y}-\widehat{\boldsymbol{y}}\|^{2}
\]</span> 第𝑙层第𝑗个神经元和第𝑙 − 1层神经元之间的关系: <span
class="math display">\[
z_{j}^{l}=\sum_{k=1} w_{j k}^{l-1} h_{k}^{l-1}+b_{j}^{l-1}, \quad
h_{j}^{l}=\sigma\left(z_{j}^{l}\right)
\]</span></p>
<h3 id="反向传播四个方程">🐮反向传播四个方程</h3>
<h4 id="bp1">BP1</h4>
<p>输出层（最后一层，即为L层）误差的方程 <span class="math display">\[
E=\frac{1}{2}\left\|\boldsymbol{y}-\boldsymbol{h}^{L}\right\|^{2}=\frac{1}{2}
\sum_{j=1}^{K}\left(h_{j}^{L}-y_{j}\right)^{2}
\]</span> 第L层第j个神经元的误差 <span class="math display">\[
\delta_{j}^{L}=\frac{\partial E}{\partial z_{j}^{L}}=\frac{\partial
E}{\partial h_{j}^{L}} \frac{\partial h_{j}^{L}}{\partial
z_{j}^{L}}=\left(h_{j}^{L}-y_{j}\right)
\sigma^{\prime}\left(z_{j}^{L}\right)
\]</span> 向量表达形式： <span class="math display">\[
𝛿_𝐿=(𝒉^𝐿−𝑦)\odot𝜎^{&#39;}(𝒛^𝐿)
\]</span></p>
<h4 id="bp2">BP2</h4>
<p>每一层的误差，使用下一层的误差 $^{l+1} $表示当前层的误差 $^l $: <span
class="math display">\[
\delta^{l}=\sigma^{\prime}\left(\mathbf{z}^{l}\right)
\odot\left(\boldsymbol{W}^{l} \delta^{l+1}\right)
\]</span></p>
<h4 id="bp3">BP3</h4>
<p>代价函数关于偏置b的偏导</p>
<h4 id="bp4">BP4</h4>
<p>代价函数关于权重的偏导</p>
<h4 id="summary">Summary</h4>
<p>向量形式： <span class="math display">\[
\begin{array}{l}\text { (BP1) }
\delta^{L}=\left(\boldsymbol{h}^{L}-y\right) \odot
\sigma^{\prime}\left(\mathbf{z}^{L}\right)\\ \text { (BP2) }
\delta^{l}=\sigma^{\prime}\left(\mathbf{z}^{l}\right)
\odot\left(\boldsymbol{W}^{l} \delta^{l+1}\right) \\ \text { (BP3) }
\frac{\partial E}{\partial b^{l-1}}=\delta^{l} \\ \text { (BP4) }
\frac{\partial E}{\partial
W^{l-1}}=\boldsymbol{h}^{l-1}\left(\delta^{l}\right)^{T}\end{array}
\]</span></p>
<p>数学形式： <span class="math display">\[
BP1:&amp; \delta_{j}^{L}=\left(h_{j}^{L}-y_{j}\right)
\sigma^{\prime}\left(z_{j}^{L}\right)\\BP2:&amp;\delta_{j}^{l}=\sum_{k=1}
\delta_{k}^{l+1} w_{k j}^{l}
\sigma^{\prime}\left(z_{j}^{l}\right)\\BP3:&amp;\frac{\partial
E}{\partial b_{j}^{l-1}}=\delta_{j}^{l}\\BP4:&amp;\frac{\partial
E}{\partial w_{j k}^{l-1}}=h_{k}^{l-1} \delta_{j}^{l}
\]</span></p>
<p>反向传播算法 1. 输入x：为输入层设置对应的激活值h1 2.
前向传播：线性组合+非线形激活 3. 输出层误差和反向误差传播：BP1和BP2 4.
输出：误差函数的梯度由BP3和BP4给出</p>
<h2 id="模型改进">模型改进</h2>
<h3 id="改进损失函数">改进损失函数</h3>
<p>对数似然</p>
<h4 id="损失函数对比">损失函数对比</h4>
<p>交叉熵VS二次代价函数</p>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 45%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>二次代价函数</th>
<th>交叉熵</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>函数表达式</td>
<td><span class="math inline">\(E = \frac{1}{2}(y-\hat
y)^2\)</span></td>
<td><span class="math inline">\(E = -yln\hat y-(1-y)ln(1-\hat
y)\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>对参数w偏导</td>
<td><span class="math inline">\(\frac{\partial E}{\partial
w}=(\hat{y}-y) \sigma^{\prime}(z) x=(\sigma(z)-y) \sigma^{\prime}(z)
x\)</span></td>
<td><span class="math inline">\(\frac{\partial E}{\partial
w}=(\hat{y}-y) x=(\sigma(z)-y) x\)</span></td>
</tr>
<tr class="odd">
<td>对参数b的求导</td>
<td><span class="math inline">\(\frac{\partial E}{\partial
b}=(\hat{y}-y) \sigma^{\prime}(z)=(\sigma(z)-y)
\sigma^{\prime}(z)\)</span></td>
<td><span class="math inline">\(\frac{\partial E}{\partial
b}=(\hat{y}-y)=(\sigma(z)-y)\)</span></td>
</tr>
</tbody>
</table>
<h3 id="权重初始化建议">权重初始化建议</h3>
<p>随机初始化:使用Numpy的 np.random.randn函数生成均
值为0，标准差为1的高斯分布。</p>
<p>改进:对于任意𝑙层，使用均值为0，标准差为1
的高斯分布随机分布初始化权重参数𝑊𝑙−1，𝑏𝑙−1。此时中间变量𝑧𝑙
服从均值为0，标准差为1的高斯分布。</p>
<h3 id="减少过拟合dropout">减少过拟合:dropout</h3>
<ol type="1">
<li>随机地删除网络中的一半的隐藏神经元，同时让输入层和输出层的神经元保持不变。</li>
<li>把输入x通过修改后的网络前向传播，然后把得到的损失结
果通过修改的网络反向传播。在mini-batch 上执行完这个过
程后，在没有被删除的神经元上更新对应的参数(w，b)</li>
<li>继续重复上述过程:
<ul>
<li>恢复被删掉的神经元(此时被删除的神经元保持原样，
而没有被删除的神经元已经有所更新)</li>
<li>从隐藏层神经元中随机选择一个一半大小的子集临时
删除掉(备份被删除神经元的参数)。</li>
<li>对一小批训练样本，先前向传播然后反向传播损失并 更新参数(w，b)
(没有被删除的那一部分参数得到
更新，删除的神经元参数保持被删除前的结果)。</li>
</ul></li>
</ol>
<h3 id="缓解梯度消失relu">缓解梯度消失:ReLU</h3>
<p>当𝑧 是负数的时候，梯度为0，神经元停止学习(类似于
dropout作用，减少过拟合);当𝑧大于0时，梯度为1，可 以缓解下溢问题</p>
<h1 id="svm-支持向量机">SVM-支持向量机</h1>
<h2 id="线形可分-svm">线形可分-SVM</h2>
<p>约束优化问题</p>
<ol type="1">
<li><p>目标函数 <span class="math inline">\(minf(x)\)</span></p></li>
<li><p>变量</p></li>
<li><p>约束条件 <span class="math display">\[
\begin{array}{lllll}\text { s.t. } &amp; g_{j}(x)=0, &amp; j=1, &amp; 2,
&amp; \cdots &amp; n \\ &amp; h_{i}(x) \leq 0, &amp; i=1, &amp; 2, &amp;
\cdots, &amp; m\end{array}
\]</span></p></li>
</ol>
<p>求解方法---拉格朗日乘数法 <span class="math display">\[
L(x, \lambda, \alpha)=f(x)+\sum_{j} a_{j} g_{j}(x)+\sum_{i} \lambda_{i}
h_{i}(x)
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial x} = \nabla f\left(x^{*}\right)+\sum_{i}
a_{j} \nabla g_{j}\left(x^{*}\right)+\sum_{i} \lambda_{i} \nabla
h_{i}\left(x^{*}\right)=0
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}\nabla_{\mathbf{x}} L=\frac{\partial L}{\partial
\mathbf{x}}=\nabla f+\lambda \nabla g=\mathbf{0} \\ \nabla_{\lambda}
L=\frac{\partial L}{\partial \lambda}=g(\mathbf{x})=0\end{array}
\]</span> 计算 L 对 x 与 <span class="math inline">\(\lambda\)</span>
的偏导数并设为零，可得最优解的必要条件 如何理解KKT？</p>
<p>Karush-Kuhn-Tucker (KKT)条件</p>
<p>非线性规划最佳解的必要条件--KKT条件将Lagrange乘数法所处理涉及等式的约束优化问题推广至不等式
<span class="math display">\[
\begin{array}{c}\nabla f\left(x^{*}\right)+\sum_{j} a_{j} \nabla
g_{j}\left(x^{*}\right)+\sum_{i} \lambda_{i} \nabla
h_{i}\left(x^{*}\right)=0 \\ g_{j}\left(x^{*}\right)=0 \\
h_{i}\left(x^{*}\right) \leq 0, \lambda_{i} \geq 0, \lambda_{i}
h_{i}\left(x^{*}\right)=0\end{array}
\]</span> ### 对偶问题 <span class="math display">\[
\max _{\alpha_{i} \geq 0} \min _{w} L(w, \alpha)
\]</span> <span class="math display">\[
f_{0}(w) = \max _{\alpha_{i} \geq 0} L(w, \alpha)\\
f_{0}(w) &gt; L(w, \alpha)
\]</span></p>
<p>简单的例子 <span class="math display">\[
\begin{array}{l}\min _{u} u^{2} \\ \text { s.t. } u&gt;=b\end{array}
\]</span> 使用拉格朗日乘数法将其转化为 <span class="math display">\[
L = u^2 + \lambda (u-b) \\
\frac{\partial L}{\partial u} = 2u + \lambda
\\ u-b = 0
\\u = b
\\\lambda = 2b
\]</span></p>
<h3 id="margin模型">Margin模型</h3>
<p><img src="/Users/wangjing/Library/Application Support/typora-user-images/image-20211116103423027.png" alt="image-20211116103423027" style="zoom:50%;" /></p>
<p>分类面： <span class="math display">\[
w^{T} x+b=0
\]</span> +1支持面： <span class="math display">\[
w^{T} x+b = 1
\]</span> -1支持面： <span class="math display">\[
w^{T} x+b=-1
\]</span> 向量 w 与支持面、分类面正交 <span class="math display">\[
\left.\begin{array}{l}w^{T} x_{1}+b=1 \\ w^{T}
x_{2}+b=1\end{array}\right\} \Rightarrow w^{T}\left(x_{1}-x_{2}\right)=0
\]</span> 使用 w 和 b 对 M 建模 <span class="math display">\[
\left.\begin{array}{l}w^{T} x^{+}+b=+1 \\ w^{T}
x^{-}+b=-1\end{array}\right\} \Rightarrow
w^{T}\left(x^{+}-x^{-}\right)=2
\]</span> 得到两个支撑面最大间隔 <span class="math display">\[
\operatorname{margin} M=\left\|x^{+}-x^{-}\right\|=\frac{2}{\|w\|}
\]</span></p>
<h3 id="分类模型">分类模型</h3>
<p>目标函数：间隔最大 （二次函数）</p>
<p><span class="math display">\[
\max \left(\frac{2}{\|w\|}\right) \Leftrightarrow \min
\left(\|w\|^{2}\right) \\ \min _{\boldsymbol{w}, b} \boldsymbol{w}^{T}
\boldsymbol{w} / 2
\]</span> 约束:线形约束 <span class="math display">\[
\left\{\begin{array}{ll}\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b \geq 1
&amp; y_{i}=1 \\ \boldsymbol{w}^{T} \boldsymbol{x}_{i}+b \leq-1 &amp;
y_{i}=-1\end{array}\right.
\]</span> 约束可合并为： <span class="math display">\[
y_{i}\left(\boldsymbol{w}^{T} x_{i}+b\right) \geq 1
\]</span></p>
<h2 id="svm--进阶">SVM--进阶</h2>
<h3 id="线形不可分svm-软-svm">线形不可分SVM-软-SVM</h3>
<p>新的优化问题 <span class="math display">\[
\min _{w, b} w^{T} w / 2+C \sum_{i=1}^{n} \epsilon_{i}
\]</span> 约束： <span class="math display">\[
\begin{array}{c}y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\epsilon_{i} \\
\epsilon_{i} \geq 0\end{array}
\]</span> 软-SVM对偶问题 <span class="math display">\[
\begin{array}{l}\max _{\alpha} \sum_{i} \alpha_{i}-\frac{1}{2} \sum_{i,
j} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{T} \mathbf{x}_{j}
\\ \sum_{i} \alpha_{i} y_{i}=0 \\ C \geq \alpha_{i} \geq 0, \forall
i\end{array}
\]</span> 用拉格朗日乘数法转华为无约束问题： <span
class="math display">\[
L=\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{n} \epsilon_{i}-\sum_{i=1}^{n}
\alpha_{i}\left(y_{i}\left(w^{T}
x_{i}+b\right)-1+\epsilon_{i}\right)-\sum_{i=1}^{n} \mu_{i} \epsilon_{i}
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial w} = 0 \\
\frac{\partial L}{\partial \alpha} = 0 \\
\frac{\partial L}{\partial \mu} = 0
\]</span></p>
<p>支持向量有两类：</p>
<ol type="1">
<li>支持面上的点</li>
<li>违背硬约束样本点</li>
</ol>
<h3 id="非线形-核svm">非线形-核SVM</h3>
<p>模型：</p>
<ol type="1">
<li>利用非线性映射把原始数据映射到高维空间中<span
class="math inline">\(\phi(x)\)</span></li>
<li>目标函数</li>
</ol>
<p><span class="math display">\[
\begin{array}{l}\min w^{T} w / 2 \\ \text { s.t. } y_{i}\left(w^{T}
\phi\left(x_{i}\right)+b\right) \geq 1\end{array}
\]</span></p>
<h3 id="多分类svm">多分类SVM</h3>
<ol type="1">
<li>一对多one-verus-rest</li>
</ol>
<p>一种正样本，多种负样本</p>
<p>会出现数据不平衡，分类面偏置 <span class="math display">\[
\hat y \leftarrow argmaxw_k x + b_k
\]</span> 改进：期望正类和负类之间的错误达到平衡 <span
class="math display">\[
\begin{array}{c}\min w^{T} w / 2+\mathrm{C}\left(\frac{N}{N_{+}}
\sum_{i: y_{i}=+1} \epsilon_{i}+\frac{N}{N_{-}} \sum_{i: y_{i}=-1}
\epsilon_{i}\right) N=N_{+}+N_{-} \\ \text {s.t. } \quad
y_{i}\left(w^{T} x_{i}+b\right) \geq 1-\epsilon_{i} \\ \epsilon_{i} \geq
0\end{array}
\]</span></p>
<ol start="2" type="1">
<li>多个1V1 one-verus-one 训练 <span
class="math inline">\(\frac{m(m-1)}{2}\)</span>个分类器</li>
</ol>
<p>样本量较少，分类器数量更多，测试成本高</p>
<h2 id="svr-支持向量回归">🍑SVR 支持向量回归</h2>
<p>结论：</p>
<p>利用KKT条件： <span class="math display">\[
\left\{\begin{array}{c}\alpha_{i}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\xi_{i}\right)=0
\\
\hat{\alpha}_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\hat{\xi}_{i}\right)=0
\\ \xi_{i}\left(C-\alpha_{i}\right)=0 \\
\hat{\xi}_{i}\left(C-\hat{\alpha}_{i}\right)=0\end{array}\right.
\]</span></p>
<ol type="1">
<li>当0 &lt; 𝛼𝑖 &lt; 𝐶, 𝑥𝑖落在间隔带上边界</li>
<li>当𝛼𝑖 = 𝐶 时，𝑥𝑖 落在间隔带上边界外侧</li>
<li>当0 &lt; 𝛼𝑖 &lt; 𝐶，𝑥𝑖落在间隔带下边界</li>
<li>当𝛼𝑖 = 𝐶 时，𝑥𝑖 落在间隔带下边界外侧</li>
<li>当𝛼𝑖 = 𝛼𝑖 = 0时，点落在间隔带内侧</li>
</ol>
<h1 id="特征选择和稀疏学习">特征选择和稀疏学习</h1>
<h2 id="特征选择">特征选择</h2>
<h2 id="过滤式选择">过滤式选择</h2>
<p>(Filter method) 单变量(Univariate)过滤方法:Signal-to-noise ratio
(S2N) <span class="math display">\[
\mathrm{S} 2 \mathrm{~N}=\frac{|\mu+-\mu-|}{\sigma^{+}+\sigma-}
\]</span> 多变量(Multivariate)过滤方法:Relief</p>
<p>给定训练集 𝑥 ,𝑦 ,..., 𝑥 ,𝑦 ,</p>
<p>1、对每个样本 𝑥𝑖，在同类样本中找最近邻 𝑥𝑖,h𝑖𝑡;在异类样本中寻找最近邻
𝑥𝑖,𝑚𝑖𝑠𝑠</p>
<p>2、计算对应于属性 𝑗 的统计量 <span class="math display">\[
\delta^{j}=\sum_{i}-\operatorname{diff}\left(x_{i}^{j}, x_{i, h i
t}^{j}\right)^{2}+\operatorname{diff}\left(x_{i}^{j}, x_{i, m i s
s}^{j}\right)^{2}
\]</span>
3、若𝛿𝑗大于指定阈值𝜏，选择属性𝑗;或者指定一个k值，选择统计量最大的前k
个特征</p>
<h2 id="包裹式选择">包裹式选择</h2>
<p>(Wrapper method)</p>
<p>将所有属性作为一个集合，每次从中选出部分作为训练特征。</p>
<p>NP难问题</p>
<p>寻找最优子集</p>
<p>验证集：选超参</p>
<h2 id="嵌入式选择--正则化">🍑嵌入式选择--正则化</h2>
<p>(Embedded method)</p>
<p>L1正则化 <span class="math display">\[
E=\frac{1}{2 n} \sum_{x}\left\|\boldsymbol{y}^{x}-\boldsymbol{h}^{x,
L}\right\|^{2}+\frac{\eta}{2 n} \sum_{l}\left\|w^{l}\right\|_{1}
\]</span> L2正则化 <span class="math display">\[
E=\frac{1}{2 n} \sum_{x}\left\|y^{x}-h^{x, L}\right\|^{2}+\frac{\eta}{2
n} \sum_{l}\left\|w^{l}\right\|_{2}^{2}
\]</span> 混合正则化 <span class="math display">\[
E=\frac{1}{2 n} \sum_{x}\left\|\boldsymbol{y}^{x}-\boldsymbol{h}^{x,
L}\right\|^{2}+\frac{\beta}{2 n}
\sum_{l}\left\|W^{l}\right\|_{1}+\frac{\eta}{2 n}
\sum_{l}\left\|W^{l}\right\|_{2}^{2}
\]</span></p>
<p>对偏置b不进行正则化，只对权重w进行正则化，假设<span
class="math inline">\(\sum_{i=1}^{n} x_{i}=0\)</span>, <span
class="math display">\[
\beta_{0}=\frac{1}{n} \sum_{i=1}^{n} y_{i}
\]</span></p>
<p><span class="math display">\[
\min _{\beta, \beta_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
x_{i}-\beta_{0}\right)^{2}+\lambda
\sum_{j=1}^{p}\left|\beta_{j}\right|^{q}
\]</span> ### L2-正则化</p>
<p>q=2，，使用L2范数正则化称为ridge regression，岭回归 <span
class="math display">\[
\min _{\boldsymbol{\beta}, \boldsymbol{\beta}_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
\boldsymbol{x}_{\boldsymbol{i}}-\beta_{0}\right)^{2}+\lambda
\sum_{j=1}^{p}\left|\beta_{j}\right|^{q}化为有约束形式：（why ？
为什么要这样化）
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}\min _{\boldsymbol{\beta}, \beta_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
\boldsymbol{x}_{i}-\beta_{0}\right)^{2} \\ \text { s. t.
}\|\boldsymbol{\beta}\|^{2} \leq t\end{array}
\]</span> 求解方法：化为矩阵形式，利用正规方程法对其进行求解 <span
class="math display">\[
L = \|Y-X \beta\|^{2} +\lambda||\beta||^2
\]</span> 对权重参数求偏导，二范数的矩阵表示 <span
class="math display">\[
\|x\|_{2}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}=\sqrt{\mathbf{X}^T \mathbf{X}}
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial \beta}=2 X^T(X \beta-Y)+2 \lambda \beta=0
\]</span></p>
<p><span class="math display">\[
\Rightarrow X^T X \beta-X^T Y+\lambda \beta=0
\]</span></p>
<p><span class="math display">\[
\Rightarrow \left(X^TX+\lambda\right) \beta=X^T Y
\]</span> 求解得到： <span class="math display">\[
\Rightarrow \beta=\left(X^TX+\lambda I\right)^{-1} X^T Y
\]</span> #### （*）SVD 奇异值分解</p>
<p>用SVD解释岭回归: <span
class="math inline">\(𝑋=𝑈𝐷𝑉^𝑇\)</span>，𝑈为𝑛×𝑝,𝑉为𝑝×𝑝正交矩
阵，𝐷为对角阵，满足𝑑1 ≥𝑑2 ≥⋯≥𝑑𝑝 ≥0 <span class="math display">\[
\begin{aligned} \mathbf{X} \hat{\beta}^{\mathrm{ls}}
&amp;=\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1}
\mathbf{X}^{T} \mathbf{y} \\ &amp;=\mathbf{U} \mathbf{U}^{T} \mathbf{y}
\\ \mathbf{X} \hat{\beta}^{\text {ridge }}
&amp;=\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}+\lambda
\mathbf{I}\right)^{-1} \mathbf{X}^{T} \mathbf{y} \\ &amp;=\mathbf{U}
\mathbf{D}\left(\mathbf{D}^{2}+\lambda \mathbf{I}\right)^{-1} \mathbf{D}
\mathbf{U}^{T} \mathbf{y} \\ &amp;=\sum_{j=1}^{p} \mathbf{u}_{j}
\frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \mathbf{u}_{j}^{T} \mathbf{y}
\end{aligned}
\]</span> ### L1-正则化</p>
<p>Least Absolute Shrinkage and Selection Operator, Lasso回归</p>
<p>q=1，w变成0，自动放弃特征，起到特征选择，防止过拟合的方法，可以使得特征矩阵稀疏。
<span class="math display">\[
\begin{array}{l}\min _{\boldsymbol{\beta}, \boldsymbol{\beta}_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
\boldsymbol{x}_{\boldsymbol{i}}-\beta_{0}\right)^{2} \\ \text { s.t.
}\|\boldsymbol{\beta}\|_{1} \leq t\end{array}
\]</span>
等价拉格朗日表达形式，用拉格朗日乘数法，化有条件极值为无条件极值： <span
class="math display">\[
\min _{\boldsymbol{\beta}, \boldsymbol{\beta}_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
\boldsymbol{x}_{\boldsymbol{i}}-\beta_{0}\right)^{2}+\lambda\|\boldsymbol{\beta}\|_{1}
\]</span> L1约束使得解关于 𝒚 非线性，而且不能像岭回归那样可以得
到封闭解。</p>
<ol type="1">
<li>闭式解需要满足正交性<span class="math inline">\(X^TX=I\)</span></li>
<li>一般方法Lasso回归求解(一般情形): 坐标下降法(CoordinateDescent)
相当于每次迭代都只是更新x的一个 维度，即把该维度当做变量，剩下
的n-1个维度当作常量,通过最小化f(x) 来找到该维度对应的新的值。</li>
<li>坐标 下降法就是通过迭代地构造序列 <span class="math inline">\(x^{0},
x^{1}, x^{2},...\)</span>来求解问题，即
最终点收敛到期望的局部极小值点</li>
</ol>
<h3 id="lasso回归">Lasso回归</h3>
<p><span class="math display">\[
\min _{\boldsymbol{\beta}, \beta_{0}}
\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{\beta}^{T}
\boldsymbol{x}_{i}-\beta_{0}\right)^{2}+\lambda\|\boldsymbol{\beta}\|_{1}
\]</span> 每次针对一个属性进行更新，为什么<span
class="math inline">\(\beta_0\)</span>下面的推到没了 <span
class="math display">\[
\mathrm{L}=\sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{p} x_{i, j}
\beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|
\]</span> 完全平方式展开，L对权重参数求导 <span class="math display">\[
\frac{\partial L}{\partial \beta_{k}}=2 a_{k}+2 b_{k} \beta_{k}+\lambda
\operatorname{sign}\left(\beta_{k}\right), \\\text { where }
a_{k}=\sum_{i=1}^{n} x_{i, k}\left(\sum_{j \neq k}^{p} x_{i, j}
\beta_{j}-y_{i}\right), b_{k}=\sum_{i=1}^{n} x_{i, k}^{2}
\]</span> 利用$=0 $,得到 <span class="math display">\[
\beta_{k}=\left\{\begin{array}{l}-\frac{1}{b_{k}}\left(a_{k}-\frac{\lambda}{2}\right),
\quad a_{k}&gt;\frac{\lambda}{2} \\ 0,
\quad-\frac{\lambda}{2}&lt;a_{k}&lt;\frac{\lambda}{2} \\
-\frac{1}{b_{k}}\left(a_{k}+\frac{\lambda}{2}\right),
a_{k}&lt;-\frac{\lambda}{2}\end{array}\right.
\]</span></p>
<h2 id="稀疏表示字典学习">稀疏表示字典学习</h2>
<p>字典学习：给定数据集<span class="math inline">\({x_1, x_2, ...,
x_n}\)</span> <span class="math display">\[
\min _{B, \alpha_{i}} \sum_{i=1}^{n}\left(\left\|x_{i}-B
\alpha_{i}\right\|^{2}+\lambda\left\|\alpha_{i}\right\|_{1}\right)
\]</span></p>
<ul>
<li>其中𝑩 ∈ 𝑅𝑝×𝑘 为字典矩阵， 𝑘为字典的词汇量(通常由用户指定)， 𝜶𝒊 ∈
𝑅𝑘是样本𝒙𝑖 ∈ 𝑅𝑝 的稀疏表示。</li>
</ul>
<p>求解方法：交替优化（控制变量法）</p>
<ol type="1">
<li>固定B，优化<span
class="math inline">\(\alpha\)</span>---Lasso回归问题-------坐标下降法求解</li>
<li>固定<span
class="math inline">\(\alpha\)</span>,优化B-----线形优化-------正规方程法求解</li>
</ol>
<h1 id="集成学习">集成学习</h1>
<p>思想：民主决策，少数服从多数</p>
<p>好的集成：个体要有差异，个体精度不能太低：好而不同</p>
<p>集成的有效性： <span class="math display">\[
H(\boldsymbol{x})={\operatorname{sign}}\left(\sum_{i=1}^{T}
h_{i}(\boldsymbol{x})\right)
\]</span></p>
<p>分类错误率随着T的增大呈指数下降 <span class="math display">\[
\begin{aligned} P(\stackrel{H(\boldsymbol{x})} \neq {f(\boldsymbol{x})})
&amp;=\sum_{k=0}^{\lfloor T / 2\rfloor}\left(\begin{array}{c}T \\
k\end{array}\right)(1-\epsilon)^{k} \epsilon^{T-k} \\ &amp; \leqslant
\exp \left(-\frac{1}{2} T(1-2 \epsilon)^{2}\right) \end{aligned}
\]</span></p>
<h2 id="重采样">重采样</h2>
<p>自适应权重重置和组合</p>
<ol type="1">
<li>随机采样：bagging</li>
<li>带权采样：boosting</li>
</ol>
<h2 id="串行式">串行式</h2>
<p>特点：强依赖</p>
<p>代表性：boosting---权值逐渐变大</p>
<h3 id="boosting">Boosting</h3>
<h4 id="算法步骤">算法步骤</h4>
<ol type="1">
<li><p>给所有训练样例赋予相同的权重</p></li>
<li><p>训练第一个基本分类器</p></li>
<li><p>对分类错误的<strong>测试样例</strong>提高其权重</p></li>
<li><p>用调整过的带权<strong>训练集</strong>训练第二个基本分类器</p></li>
<li><p>重复上述过程</p></li>
<li><p>对所有的基分类器进行加权组合</p></li>
</ol>
<p><span class="math display">\[
H_{M}(x)=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m}
h_{m}(x)\right)
\]</span></p>
<p><span class="math inline">\(h_m\)</span>是基分类器，<span
class="math inline">\(w_n^m\)</span>表示样本权重，n为样本数量，m为个体分类器数</p>
<ul>
<li>对于分类错误的样本--提高其权重</li>
</ul>
<h4 id="ada-boosting">🍑Ada boosting</h4>
<p>考试会考</p>
<h5 id="模型">模型</h5>
<p>二分类问题：</p>
<p>N个训练样本：<span class="math inline">\(x_{n}(n=1, \ldots,
N)\)</span></p>
<p>每个训练样本的标签为<span class="math inline">\(y_{n} \in\{-1,+1\},
\quad h_{m}(x) \in\{-1,+1\}\)</span></p>
<h5 id="算法步骤-1">算法步骤：</h5>
<ol type="1">
<li>初始化每个训练样本的权重<span class="math inline">\(w_n\)</span>:
<span class="math inline">\(w_{n}^{(1)}=1 / N\)</span>，均匀分配</li>
<li>第一个基分类器开始训练，通过最小化误差函数<span
class="math inline">\(min L_{m}=\sum_{n=1}^{N} w_{n}^{(m)}
I\left(h_{m}\left(x_{n}\right) \neq y_{n}\right)\)</span>，<span
class="math inline">\(I\)</span>为指示函数，训练分类器<span
class="math inline">\(h_m\)</span>的参数</li>
<li>计算加权的分类错误率<span
class="math inline">\(\epsilon_{m}=\frac{\sum_{n=1}^{N} w_{n}^{(m)}
I\left(h_{m}\left(x_{n}\right) \neq y_{n}\right)}{\sum_{n=1}^{N}
w_{n}^{(m)}}\)</span>，错误率逐渐降低</li>
<li>计算分类器权重<span class="math inline">\(\alpha_{m}=\ln
\frac{1-\epsilon_{m}}{\epsilon_{m}}\)</span>，分类权重逐渐增大</li>
<li>更新样本权重<span class="math inline">\(w_{n}^{(m+1)}=w_{n}^{(m)}
\exp \left(\alpha_{m} I\left(h_{m}\left(x_{n}\right) \neq
y_{n}\right)\right)\)</span></li>
<li>使用<span class="math inline">\(H_M(x) = sign(\sum_{m=1}^M\alpha_m
h_m(x))\)</span></li>
</ol>
<h4 id="error-model-2">Error model</h4>
<h2 id="并行式">并行式</h2>
<p>不存在强依赖</p>
<h3 id="决策树">决策树</h3>
<h1 id="贝叶斯分类器">贝叶斯分类器</h1>
<p>x样本，c为标签</p>
<p>判别式模型：</p>
<p>生成式模型：</p>
<p>GAN：对抗生成网络</p>
<p>DCGAN：</p>
<h2 id="模型-1">模型</h2>
<p>基于贝叶斯公式的后验概率： <span class="math display">\[
\begin{aligned} P\left(C=c_{i} \mid \mathbf{X} = \mathbf{x}\right)
&amp;=\frac{P\left(\mathbf{X}=\mathbf{x} \mid C=c_{i}\right)
P\left(C=c_{i}\right)}{P(\mathbf{X}=\mathbf{x})} \\ &amp; \propto
P\left(\mathbf{X}=\mathbf{x} \mid C=c_{i}\right) P\left(C=c_{i}\right)
\\ &amp;  \text { for } i=1,2, \cdots, L \end{aligned}
\]</span> ## 贝叶斯分类</p>
<p><span class="math display">\[
\underset{c_{i} \in C}{\operatorname{argmax}} {P\left(x_{1}, x_{2},
\ldots, x_{p} \mid c_{j}\right) P\left(c_{j}\right)}
\]</span> ### 朴素贝叶斯分类</p>
<p>朴素条件：对已知类别，假设所有属性互相对立 <span
class="math display">\[
\begin{aligned} P\left(X_{1}, X_{2}, \cdots, X_{p} \mid C\right)
&amp;=P\left(X_{1} \mid X_{2}, \cdots, X_{p}, C\right) P\left(X_{2},
\cdots, X_{p} \mid C\right) \\ &amp;=P\left(X_{1} \mid C\right)
P\left(X_{2}, \cdots, X_{p} \mid C\right) \\ &amp;=P\left(X_{1} \mid
C\right) P\left(X_{2} \mid C\right) \cdots P\left(X_{p} \mid C\right)
\end{aligned}
\]</span> 朴素贝叶斯分类器模型（联合转化为连乘）： <span
class="math display">\[
\arg \max _{c_{j} \in C} P\left(c_{j}\right) \prod_{i=1}^{P}
P\left(x_{i} \mid c_{j}\right)
\]</span> 需要估计：</p>
<ul>
<li>先验<span class="math inline">\(𝑃 (𝐶 = 𝑐_𝑗 )\)</span></li>
<li>每个属性的条件概率<span
class="math inline">\(𝑃(𝑥_𝑖|𝑐_𝑗)\)</span></li>
</ul>
<h4 id="避免0概率问题">避免0概率问题</h4>
<p>若某个属性值在训练集中没有与某个类同时出现过，则基于频率的概率估计将为零。</p>
<p>修正：在分母上+属性取值数目，分子加上类的个数 <span
class="math display">\[
\hat{P}\left(X_{\mathrm{i}}=x \mid
C=c_{j}\right)=\frac{N\left(X_{\mathrm{i}}=x \mid
C=c_{j}\right)+1}{N\left(C=c_{j}\right)+\left|X_{i}\right|}
\]</span> ### 高斯朴素贝叶斯分类器</p>
<h4 id="高斯分布">高斯分布</h4>
<p><span class="math display">\[
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},
\boldsymbol{\Sigma})=\frac{1}{(2 \pi) \mathbf{P} / 2}
\frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \exp
\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}
\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}
\]</span> 一维Gaussian：</p>
<p>均值和方差的极大似然估计值分别是样本的均值及样本的方差 <span
class="math display">\[
\mu=\frac{1}{n} \sum_{i=1}^{n} x_{i}, \quad \sigma^{2}=\frac{1}{n}
\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}
\]</span> 多维 Gaussian： <span class="math display">\[
\mu=\frac{1}{n} \sum_{i=1}^{n} x_{i}, \quad \Sigma=\frac{1}{n}
\sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{T}
\]</span> ### 高斯朴素贝叶斯分类器</p>
<p><span class="math display">\[
\underset{C}{\operatorname{argmax}} P(C \mid
X)=\underset{C}{\operatorname{argmax}} P(X,
C)=\underset{C}{\operatorname{argmax}} P(X \mid C) P(C)
\]</span></p>
<p><span class="math display">\[
\hat{P}\left(x_{i} \mid C=c_{j}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{i
j}} \exp \left(-\frac{\left(x_{i}-\mu_{i j}\right)^{2}}{2 \sigma_{i
j}^{2}}\right)
\]</span></p>
<p>x样本，c为标签</p>
<p>由X得到高斯分布的均值和方差，后代入高斯分布</p>
<p><span class="math inline">\(P(密度\mid好瓜) =
P(密度\mid好瓜)*P(含糖率\mid好瓜)\)</span></p>
<p>朴素：用所有样本</p>
<p>非朴素：联合等于两个乘积</p>
<p><span class="math inline">\(共有 L×(p+p× (p+1)/2)个参数\)</span></p>
<p><span class="math inline">\(\sum:p×p\)</span></p>
<p><span class="math inline">\(\mu:p\)</span></p>
<p>朴素高斯必要性：估计的参数量减少</p>
<p>逻辑回归决策面：<span class="math inline">\(\theta^TX =
0\)</span></p>
<p>高斯贝叶斯决策面：</p>
<p>分到l类和k类的概率相等 <span class="math display">\[
\log P\left(c_{k} \mid x\right)-\log P\left(c_{l} \mid x\right)=0
\]</span></p>
<p>用贝叶斯公式展开 <span class="math display">\[
\log \frac{P\left(C_{k} \mid X\right)}{P\left(C_{1} \mid X\right)}=\log
\frac{P\left(X \mid C_{k}\right)}{P\left(X \mid C_{l}\right)}+\log
\frac{P\left(C_{k}\right)}{P\left(C_{l}\right)}
\]</span> 其中 <span class="math display">\[
\log P\left(x \mid
c_{k}\right)=-\frac{1}{2}\left(\mathrm{x}-{\mu}_{k}\right)^{T}
{\sum_{k}}^{-1}\left(x-\mu_{k}\right)-\log
\left|\Sigma_{k}\right|^{\frac{1}{2}}
\]</span></p>
<p><span class="math display">\[
\begin{array}{l}\log \frac{P\left(c_{k} \mid
\mathrm{x}\right)}{P\left(c_{l} \mid x\right)} \\
=\frac{1}{2}\left(\mathrm{x}-\mu_{l}\right)^{T} \Sigma_{l}{
}^{-1}\left(x-\mu_{l}\right)-\frac{1}{2}\left(\mathrm{x}-\mu_{k}\right)^{T}
\Sigma_{k}{ }^{-1}\left(x-\mu_{k}\right)+\log
\frac{\left|\Sigma_{l}\right|^{\frac{1}{2}}}{\left|\Sigma_{k}\right|^{\frac{1}{2}}}+\log
\frac{\pi_{k}}{\pi_{l}}\end{array}
\]</span></p>
<p>假设每一类的协方差矩阵均相同，</p>
<p><span class="math display">\[
\sum_{\boldsymbol{k}}=\sum, \forall \boldsymbol{k}
\]</span></p>
<p><span class="math display">\[
\Sigma_{j}=\left[\begin{array}{ccc}\sigma_{1 j}^{2} &amp; \cdots &amp; 0
\\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \sigma_{p
j}^{2}\end{array}\right]
\]</span></p>
<p>决策函数可以从x的二次转为一次函数 <span class="math display">\[
\begin{aligned}=&amp; \log
\frac{\pi_{k}}{\pi_{l}}+\frac{1}{2}\left(\mathrm{x}-\mu_{l}\right)^{T}
\Sigma^{-1}\left(x-\mu_{l}\right)-\frac{1}{2}\left(\mathrm{x}-\mu_{k}\right)^{T}
\Sigma^{-1}\left(x-\mu_{k}\right) \\=&amp; \frac{\log
\frac{\pi_{k}}{\pi_{l}}+\frac{1}{2} \mu_{l}^{T} \Sigma^{-1}
\mu_{l}-\frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k}}{\mathrm{~b}}+x^{T}
\frac{\Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)}{\mathrm{a}} \end{aligned}
\]</span> 决策边界： <span class="math display">\[
x^{T} \mathrm{a}+b=0
\]</span> 要估计的参数个数：<span class="math inline">\(L×(p+p×
(p+1)/2)\)</span></p>
<p>若 <span class="math inline">\(a_{0}+\sum_{i=1}^{p} a_{j}
x_{j}&gt;0\)</span>，将𝑥的标签置为𝑐1，否则将其标签 置为𝑐2</p>
<h2 id="lda决策面">LDA决策面</h2>
<p>通过假设每一类具有的相同协方差矩阵，得到一种经典
的线性学习方法：线性判别分析（Linear Discriminant Analysis, LDA）</p>
<p>线形决策面 <span class="math display">\[
\begin{array}{l}=\log
\frac{\pi_{k}}{\pi_{l}}+\frac{1}{2}\left(\mathrm{x}-\mu_{l}\right)^{T}
\Sigma^{-1}\left(x-\mu_{l}\right)-\frac{1}{2}\left(\mathrm{x}-\mu_{k}\right)^{T}
\Sigma^{-1}\left(x-\mu_{k}\right) \\ =\log
\frac{\pi_{k}}{\pi_{l}}+\frac{1}{2} \mu_{l}^{T} \Sigma^{-1}
\mu_{l}-\frac{1}{2} \mu_{k}^{T} \Sigma^{-1} \mu_{k}+x^{T}
{\Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)}\end{array}
\]</span></p>
<h3 id="参数估计">参数估计</h3>
<p>先验： <span class="math display">\[
\widehat{P}\left(C=C_{j}\right)=\frac{N\left(C=c_{j}\right)}{N}
\]</span> 均值：第j个高斯分布的均值 <span class="math display">\[
\mu_{j}=\frac{1}{N\left(C=c_{j}\right)} \sum_{X \in c_{j}} X
\]</span> 方差： <span class="math display">\[
\Sigma=\frac{1}{N} \sum_{c_{j} \in C} \sum_{X \in
c_{j}}\left(X-\mu_{j}\right)\left(X-\mu_{j}\right)^{T}
\]</span> 🍑求决策边界</p>
<p>已知相应的参数： <span class="math display">\[
\begin{array}{l}\begin{array}{l}* \pi_{1}=\pi_{2}=0.5 \\ *
\mu_{1}=(0,0)^{T}, \mu_{2}=(2,-2)^{T}\end{array} \\ *
\Sigma=\left(\begin{array}{cc}1.0 &amp; 0.0 \\ 0.0 &amp;
0.5625\end{array}\right)\end{array}
\]</span> 则代入上述公式求得决策边界： <span class="math display">\[
\text { *Decision boundary: } 5.56-2.00 x_{1}+3.56 x_{2}=0.0
\]</span></p>
<p>Loss function</p>
<table>
<thead>
<tr class="header">
<th>逻辑回归</th>
<th>LDA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>无𝑥 的分布假设：</td>
<td>假设𝑥服从高斯分布</td>
</tr>
</tbody>
</table>
<p>逻辑回归--判别式 <span class="math display">\[
\begin{array}{l}{L = P(c \mid x ;
\theta)}=\left(f_{\theta}(x)\right)^{c}\left(1-\frac{f_{\theta}}{\theta}(x)\right)^{1-c}
\\ \theta^{(0)} \quad
\theta^{(1)}:=\theta^{(0)}+\left.\alpha^{+\frac{e^{-\theta^{T}-x}}{\partial
\theta}}\right|_{\theta^{(0)}}\end{array}
\]</span> LDA--生成式 <span class="math display">\[
\begin{array}{l}P\left(x \mid c_{k}\right) \sim M\left(\mu_{k},
\Sigma_{h}\right) \\ =\frac{1}{(2 \pi)^{\frac{p}{2}} \mid \Sigma_{k}
\cdot \frac{1}{2}} \exp -\frac{1}{2}\left(\mathrm{x}-\mu_{k}\right)^{T}
\sum_{k}^{-1}\left(x-\mu_{k}\right) \\ \text { 其中 } \Sigma_{k}=\Sigma,
\forall k . \quad \mu_{k}=\frac{1}{1} \sum_{\frac{2}{2}} x_{i} \\ \text
{ 决策边界 线性 } \sum_{k}=\frac{1}{h} \overline{2}\end{array}
\]</span></p>
<p>高斯朴素贝叶斯决策面</p>
<h1 id="k-nn分类算法">K-NN分类算法</h1>
<p>K-近邻 （K-nearest neighbors）</p>
<p>对一个未知样本进行分类：</p>
<ol type="1">
<li><p>计算未知样本与标记样 本的距离</p></li>
<li><p>确定 k 个近邻</p></li>
<li><p>使用近邻样本的标签确定目标的标签：</p>
<p>例如， 将其划分到 k个样本中 出现最频繁的类</p></li>
</ol>
<p>没有模型，没有error model</p>
<p>KNN回归</p>
<h1 id="马尔可夫链">马尔可夫链</h1>
<p>Markov模型（链）</p>
<h2 id="贝叶斯网">贝叶斯网</h2>
<p>有向无环图</p>
<p>特征变量之间的</p>
<figure>
<img
src="/Users/wangjing/Library/Application%20Support/typora-user-images/image-20211108090515589.png"
alt="image-20211108090515589" />
<figcaption aria-hidden="true">image-20211108090515589</figcaption>
</figure>
<p>1步转移 <span class="math display">\[
\begin{array}{l}v_{t+1}(j)=P\left(X_{t+1}=j\right) \\ =\sum_{i=1}^{K}
P\left(X_{t}=i\right) P\left(X_{t+1}=j \mid
X_{t}=i\right)=\sum_{i=1}^{K} v_{t}(i) \mathrm{A}_{i j}=v_{t}
\mathrm{~A}(:, \mathrm{j})\end{array}
\]</span> n步转移 <span class="math display">\[
P({\mathrm{X}_{1}}=i_{1}, \ldots, \mathrm{X}_{T}=i_{T})=\pi_{i_{1}}
\prod_{t=2}^{T} A_{i_{t-1} i_{t}}
\]</span></p>
<h2 id="平稳分布">平稳分布</h2>
<p>平稳分布：对于一个Markov链，给定初始状态分布 𝑣1 = 𝜋 = 𝑃 𝑋1 = 1 , ...
, 𝑃 𝑋1 = 𝐾 ，利用状态转移 公式𝑣𝑡+1=𝑣𝑡A，经过一定次数的迭代之后，若能达到
෤ 𝑣= ෤ 𝑣A 则称Markov链达到了平稳分布。
一旦进入平稳分布，在之后的任意时刻的概率分布 永远为 ෤
𝑣，马尔可夫链处于稳定状态 稳定状态： ෤ 𝑣经过A转移后仍然是 ෤ 𝑣</p>
<p>应用</p>
<ol type="1">
<li>句子补全</li>
<li>网页排序：page-rank</li>
</ol>
<p>damping look 解决断链问题</p>
<p>🍑平稳分布的计算</p>
<h2 id="page-rank">page-rank</h2>
<p>PageRank认为某个网页的重要性有两个因素决定：指
向网页的链接数量以及输出网页的链接数量。</p>
<p>超链接的个数和质量</p>
<p>数量假设和质量假设</p>
<p>若网页𝑗到网页𝑖有边，则令 <span class="math inline">\(𝐿_{𝑖𝑗} =
1\)</span>，否则 <span class="math inline">\(𝐿_{𝑖𝑗} = 0\)</span>。因
此， <span
class="math inline">\(𝑗\)</span>的输出链接为，出链的个数（有向图出度）
<span class="math display">\[
c_{j}=\sum_{i=1}^{N} L_{i j}
\]</span> 指向网页的链接越多权重越大，而输出网页的链接越多权 重越小
<span class="math display">\[
p_{i}=(1-d)+d \sum_{j=1}^{N}\left(\frac{L_{i j}}{c_{j}}\right) p_{j}
\]</span> <span class="math inline">\(p_i\)</span>为为网页重要性，<span
class="math inline">\(c_j\)</span>表示网页j对网页i的重要性的程度</p>
<p>🍑参数估计计算</p>
<p>MLE最大似然估计for Markov chain</p>
<h2 id="hmm隐马尔可夫链">HMM隐马尔可夫链</h2>
<p>HMM三个问题</p>
<ol type="1">
<li><p>评估问题：概率计算问题</p>
<p>估计模型下观测序列出现的概率</p></li>
<li><p>解码问题：状态预测问题</p>
<p>给定模型参数和一个观测序列，推断隐状态 序列</p></li>
<li><p>学习问题：参数估计问题</p>
<p>给定多个观测数据Y，估计一组参数</p></li>
</ol>
<figure>
<img
src="/Users/wangjing/Library/Application%20Support/typora-user-images/image-20211108110010355.png"
alt="image-20211108110010355" />
<figcaption aria-hidden="true">image-20211108110010355</figcaption>
</figure>
<p>常规方法：遍历--复杂度指数级</p>
<h1 id="非监督学习">非监督学习</h1>
<p>--压缩思想 1. 纵向结构--聚类 2. 横向结构--降维度</p>
<p>线形： 非线形：</p>
<h2 id="聚类">聚类</h2>
<p>clustering---簇内距小，簇间距大 簇的定义 数据表示：向量空间
相似性/距离：欧氏距离 簇的个数：数据驱动，自己识别出来
聚类算法：划分聚类算法，层次聚类算法 算法的收敛性：收敛速度</p>
<p>层次式聚类算法 1. 自顶向上：聚合 2. 自顶向下：分裂</p>
<p>⭐️划分式聚类算法 1. K-means 2. GMM（高斯混合模型）</p>
<h2 id="k-means">K-means</h2>
<h3 id="模型-2">模型</h3>
<p>算法步骤：</p>
<p>输入：数据N个样本，簇的个数指定为K</p>
<ol type="1">
<li>初始化：随机选择K个数据点作为相应的簇中心{}</li>
<li>迭代：
<ol type="1">
<li>对每一个样本西交进行归簇，距离哪个聚类中心最近，则讲其归为哪一簇
<span class="math inline">\(x_{j} \in C_{i} \Leftrightarrow \min _{t=1,
\ldots,
K}\left\{\left\|x_{j}-\mu_{t}\right\|\right\}=\left\|x_{j}-\mu_{i}\right\|\)</span></li>
<li>重新计算每个簇的均值（簇中心）<span
class="math inline">\(\mu_{i}=\frac{1}{\left|C_{i}\right|} \sum_{x_{j}
\in C_{i}} x_{j}\)</span></li>
</ol></li>
<li>终止迭代：簇中心不发生改变时</li>
</ol>
<p>输出：簇中心</p>
<p>目标函数：簇内样本到簇中心的平方和距离最小 <span
class="math display">\[
\operatorname{argmin}_{C, \mu} \sum_{i=1}^{K} \sum_{x_{j} \in
C_{i}}\left\|x_{j}-\mu_{i}\right\|_{2}^{2}
\]</span> 非凸函数，NP-hard</p>
<p>解决之道：迭代优化（交替优化：固定一组变量值去优化另一组变量值）</p>
<p>• 初始化K个簇中心:𝜇 = {𝜇1, 𝜇2, ... , 𝜇𝐾} • 迭代进行以下优化</p>
<p>• 更新簇成员:固定𝜇，优化𝐶 • 更新簇中心:固定𝐶，优化𝜇</p>
<h4 id="算法复杂度">算法复杂度：</h4>
<p>迭代次数:假设迭代 𝑙 步算法收敛。因此总的计算复杂度</p>
<p>为 O(𝑙 𝐾np) 由于𝐾和𝑙通常都远远小于n，可认为是关于n 的线性复杂度</p>
<h4 id="初值对算法的影响">初值对算法的影响：</h4>
<p>通过启发式方法选择好的初值:例如要求种子点之间有较大的距离
尝试多个初值，选择平方误差和最小的一组聚类结果</p>
<h4 id="聚类数目k的影响">聚类数目K的影响：</h4>
<p>手肘法:目标函数的值和 k 的关系图是一个手肘的形状，而这个肘部
对应的k值就是数据的最佳聚类数。k=2时，对应肘部，故选择 k值为2</p>
<h4 id="局限性">局限性</h4>
<p>不适合对形状不是超维椭圆体(或超维球体)的数据</p>
<h2 id="k-means延伸">K-means延伸</h2>
<p>层次-kmeans</p>
<p>乘积量化：就是划分数据集，然后分别划分出聚类中心，然后向量乘积</p>
<p>128*n</p>
<p>n个样本训练256个cluster：聚类中心</p>
<p>划分样本集做完笛卡尔乘积之后的维数和用原始样本求中心的维数一样吗？</p>
<h1 id="pca">PCA</h1>
<h2 id="标准pca">标准PCA</h2>
<p>线形PCA</p>
<h3 id="三种建模思想">三种建模思想</h3>
<p>PCA 求解角度</p>
<ol type="1">
<li>最大投影方差</li>
<li>最小投影距离</li>
<li>奇异值分解(SVD)</li>
</ol>
<h4 id="最大投影方差">最大投影方差</h4>
<p>信息（方差）能尽可能大的保持</p>
<h4 id="最小投影距离">最小投影距离</h4>
<p>投影数据与原数据的之间的最小平方距离尽可能小</p>
<p>目标函数： <span class="math display">\[
\mathbf{w}_{1}=\arg \max _{|\mathbf{w}|-1} \frac{1}{m}
\sum_{i=1}^{m}\left\{\left(\mathbf{w}^{T}
\mathbf{x}_{i}\right)^{2}\right\} \quad
\operatorname{Var}(X)=E\left\{[X-E(X)]^{2}\right\}
\]</span></p>
<p><span class="math display">\[
L=-w^{\top} A w+\lambda\left(w^{\top} w-1\right)\\
\frac{\partial L}{\partial w}=-2 \cdot A w+2 \lambda w=0 \Rightarrow A
w=\lambda w \Rightarrow w^{\top} A w=\lambda
\]</span></p>
<p>w1协方差矩阵的最大特征值对应的特征向量 <span class="math display">\[
\max _{W \in R^{p * k}} \operatorname{tr}\left(W^{T}\left(\frac{1}{m} X
X^{T}\right) W\right), W^{T} W=I_{k}
\]</span> PCA主方向 = 数据协方差矩阵的特征向量 •
更大特征值对应的特征向量更加重要 降维结果</p>
<p><span class="math display">\[
Z=W^{T} X
\]</span> 重构结果 <span class="math display">\[
\widehat{\mathrm{X}}=W Z=W W^{T} X
\]</span></p>
<p>目标:计算数据k个主方向</p>
<ul>
<li><p>第一步:数据居中</p></li>
<li><p>第二步:计算居中数据的协方差矩阵</p></li>
<li><p>第三步:计算协方差矩阵最大k个特征值对应的特征
向量，组成矩阵</p></li>
<li><p>输出降维结果</p></li>
<li><p>问题:</p>
<p>• 第k个主成分的方差是多少?</p>
<p>• k 选择多大 <span class="math display">\[
\begin{array}{l}w_{k}^{T}\left(\frac{1}{M} \sum_{i=1}^{M} x_{i}
x_{i}^{T}\right) w_{k} \\ =w_{k}^{T} \lambda_{k} w_{k}=\lambda_{k}
w_{k}^{T} w_{k}=\lambda_{k}\end{array}
\]</span> K的选择</p></li>
</ul>
<h3 id="奇异值分解svd">奇异值分解SVD</h3>
<p><span class="math display">\[
A=U \Sigma V^{T}
\]</span></p>
<h4 id="pca应用-数据预处理">PCA应用-数据预处理</h4>
<p>数据白化(Whitening)操作</p>
<p>使用PCA，可以同时去除变量之间的线性关系以及对数据进行归一化:</p>
<ul>
<li><p>假设数据的协方差矩阵为S <span class="math display">\[
S=\frac{1}{m} \sum_{i=1}^m(x_{i}-\bar{x})(x_{i}-\bar{x})^{T}
\]</span></p></li>
<li><p>利用<span class="math inline">\(W^{T} S
W=\Lambda\)</span>定义一个变换 <span class="math display">\[
y_{i}=\Lambda^{-\frac{1}{2}} W^{T}\left(x_{i}-\bar{x}\right)
\]</span></p></li>
</ul>
<p>则y的均值为0，协方差为单位矩阵。</p>
<h2 id="概率pca">概率PCA</h2>
<h2 id="核pca">核PCA</h2>
<h3 id="步骤">步骤</h3>
<ol type="1">
<li>输入</li>
<li>构造Gram矩阵</li>
<li>对高维数据去中心化</li>
<li>对K进行特征分解</li>
<li>计算x的低维表示</li>
</ol>
<h2 id="lle">LLE</h2>
<p>Locally Linear embedding 局部线性嵌入</p>
<p>LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征</p>
<ol type="1">
<li>找最近邻：欧氏距离</li>
<li>重构：重构系数之和=1</li>
</ol>
<p><span class="math display">\[
\begin{aligned} \varepsilon(W)
&amp;=\sum_{i=1}^{N}\left\|x_{i}-\sum_{j=1}^{k} W_{i j} x_{i
j}\right\|^{2} =\sum_{i=1}^{N}\left\|\sum_{j=1}^{k} W_{i
j}\left(x_{i}-x_{i j}\right)\right\|^{2} \end{aligned}
\]</span> <span
class="math inline">\(W_{ij}\)</span>求解方法：拉格朗日乘数法 <span
class="math display">\[
L\left(W_{i}\right)=W_{i}^{T} Z_{i} W_{i}+\lambda\left(W_{i}^{T} 1_{k
\times 1}-1\right)
\]</span></p>
<p><span class="math display">\[
2 Z_{i} W_{i}+\lambda 1_{k \times 1}=0, \text { 即 }
W_{i}=-\frac{\lambda}{2} Z_{i}^{-1} 1_{k \times 1}
\]</span></p>
<ol start="3" type="1">
<li>低维嵌入🍑</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/12/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Author">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SweetieeWang's Page">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/12/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-04-12 13:41:42" itemprop="dateCreated datePublished" datetime="2023-04-12T13:41:42+08:00">2023-04-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Author</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Author</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
